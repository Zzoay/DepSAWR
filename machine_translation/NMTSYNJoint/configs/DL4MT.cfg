[Data]
data_dir = /data/private/zms/DEPSAWR/wmt18_data/final_data
train_files = %(data_dir)s/joint_bpe/corpus.tc.en %(data_dir)s/joint_bpe/corpus.tc.de
# train_files = %(data_dir)s/joint_bpe/newstest2018.tc.en %(data_dir)s/base/newstest2018.tc.de
dev_files = %(data_dir)s/joint_bpe/newstest2018.tc.en %(data_dir)s/base/newstest2018.tc.de
test_files = %(data_dir)s/joint_bpe/newstest2018.tc.en %(data_dir)s/base/newstest2018.tc.de

[Save]
save_dir = save/lstm_18
config_file = %(save_dir)s/config.cfg
load_dir = save/lstm_18
load_model_path = %(load_dir)s/model
num_kept_checkpoints = 5

[Network]
# DL4MT or Transformer
model_name = DL4MT
src_vocab_size = 20000
tgt_vocab_size = 30000
num_layers = 6
num_heads = 8
embed_size = 512
lstm_hidden_size = 1024
hidden_size = 1024
attention_size = 2048
dropout_emb = 0.0
dropout_lstm_input = 0.0
dropout_lstm_hidden = 0.0
dropout_hidden = 0.0
param_init = 0.1
add_position_emb = True
proj_share_weight = False
bridge_type = mlp
label_smoothing = 0.0

[Optimizer]
learning_algorithm = adam
learning_rate = 0.0005
start_decay_at = 0
decay_scale = 0.75
decay_method = loss
decay_steps = 10
beta_1 = .9
beta_2 = .98
epsilon = 1e-12
clip = 5.0
schedule_method = loss
max_patience = 4
min_lrate = 0.00005

[Run]
train_iters = 50000
batching_key = sample
train_batch_size = 50
test_batch_size = 8
validate_every = 500
update_every = 10
save_after = 0
eval_start = 0
decode_max_time_step = 100
max_train_length = 100
max_src_length = 100
max_tgt_length = 100
beam_size = 5
bleu_script = multi-bleu.pl

[Vocab]
src_vocab_type = word
tgt_vocab_type = word
src_vocab_path = /data/private/zms/DEPSAWR/wmt18_data/final_data/joint_bpe/jvocab.en.json
tgt_vocab_path = /data/private/zms/DEPSAWR/wmt18_data/final_data/joint_bpe/jvocab.de.json
extention_vocabs_path = {'weight':'./debug/en.json'}
extention_embeddings_path = {'weight':'./save/test.ckpt'}
extention_embeddings_size = {'weight':512}
src_fusion_list = ['weight']
tgt_fusion_list = []
load_extention_vocab = False