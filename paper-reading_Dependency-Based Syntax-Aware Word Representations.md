# Dependency-Based Syntax-Aware Word Representations

## 'abstract'

- what is Dependency-Based, Syntax-Aware?

  Dependency parsing output the 1-best dependency syntax, but it suffer from error propagation, so this paper proposed to use the hidden representations. The key idea is to **use the intermediate hidden representations of a well-trained encoder-decoder dependency parser**, which are referred to as **Dependency-based Syntax-Aware Word Representations (DepSAWRs)**. 

- methods? 

  The key idea is to **use the intermediate hidden representations of a well-trained encoder-decoder dependency parser.** Then, we simply **concatenate** them with the conventional context-insensitive word embeddings to compose input word representations, without requiring to modify the model architecture of the downstream tasks.

- tasks?  

  sentence classification, sentence matching, sequence labeling and machine translation

- contributions

  - bringing consistently better performance on the four tasks
  - outperform the Tree-RNN and Tree-Linearization approaches in most settings, meanwhile highly efficient in syntax integration
  - easily extendable to encoding other structural attributes of language



## Dependency Syntax Representation

### Tree-RNN

We extend the gated recurrent unit (GRU) as the basic composition operation for Tree-RNN. Given a dependency tree as input, we compute the bottom-up and top-down representations by the following equations:

<img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20201106140001290.png" alt="image-20201106140001290" style="zoom:67%;" />

where $x_i$ is the concatenation of word embedding and dependency label embedding, $W,U,V$ are the model parameters, $LC, RC$ are the left and right children. Then we concatenate the bottom-up and top-down outputs  as the dependency syntax features.

### Tree-Linearization

The basic idea is to factorize a parse tree into a sequence of shift-reduce actions.

<img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20201106103231502.png" alt="image-20201106103231502" style="zoom:50%;" />

Then, we use the sequence as input to the downstream tasks. For the baseline, we use word embeddings as input. While for the expoliting pre-trained model like ELMo and BERT,  SH symbols represented as the word vector generated by them, while the other symbols are represented as embedding. 

- Basline: word embedding
- ELMo and BERT: 
  - 'SH' - > Pre-trained word representations
  - Other -> Word embedding

### Dep-SAWR

We exploit the BiAffine dependency parser to produce the dependency syntax inputs.

<img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20201106124307456.png" alt="image-20201106124307456" style="zoom:67%;" />

- Encoder: Input -> Embedding -> BiLSTM * 3 -> parallel MLP * 2  

- Decoder: the rest BiAffine scorer

The key operation is to convert the encoder outputs of a dependency parsing model into a sequence of vectors by a projection module:

<img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20201106124755449.png" alt="image-20201106124755449" style="zoom:67%;" />

where $h_l = h^{syn-lstm} \cup h^{head} \cup h^{child}$ , a total of 5 layers, and $W_l, \lambda_l, b_l$ are the model parameters.

Then, we concatenate the projected outputs with input vectors of the baseline target model, and feed the new representations into the remaining network structure for the target tasks. 

<img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20201105230852331.png" alt="image-20201105230852331" style="zoom:64%;" />



### Syntax Integration

We make integrations only at the word representation part, keeping the other part of the target baseline model unchanged. 

- Dep-SAWR: The outputs of Dep-SAWR concatenated with the original word representations as the new inputs. (Figure 5)
- Tree-RNN and Tree-Linearization: Use the hiddent vectors as input directly.



## Tasks

sentence classification, sentence matching, sequence labeling and machine translation

### sentence classification

<img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20201105230030385.png" alt="image-20201105230030385" style="zoom:67%;" />



### sentence matching

<img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20201105230643218.png" alt="image-20201105230643218" style="zoom:67%;" />



### sequence labeling

<img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20201105230736173.png" alt="image-20201105230736173" style="zoom:67%;" />

### machine translation

<img src="C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\image-20201105230805259.png" alt="image-20201105230805259" style="zoom:67%;" />



## Experiments

### Common Settings

**Baseline word representations**

- English: Glove, ELMo, BERT

- Chinese: pretrained 300-dim word embedding on the Chinese Gigaword Third Edition, ELMo, BERT

- German: pretrained 300-dim word vectors released by fastText, ELMo, BERT



**Syntax-aware word representations generation** To obtain the syntax-aware word representations, we train three individual parsers for the English, Chinese and German, respectively. All parsers are trained by using the BiAffine dependency parsing model, by using the same setting as Dozat. The trainning datasets as below:

- English: English PennTreebank dataset with Stanford Dependencies 3.5.0
- Chinese: Chinese Treebank 7.0 with Stanford dependencies 3.5.0
- German: German universal dependency treebank, version 2.2



**Hyperparameter**

- optimizer: Adam

- learning rate: 5*10e-4

- gradient clipping threshold: 10

- mini-batch size: 32

- dropout ratio: 0.5



### Sentence Classification

Baseline model:

- word embeddings (Glove and so on)
- ELMo
- BERT



## Code

https://github.com/zhangmeishan/DepSAWR



## Issues

1. The parsing model is well-trained, and the parameters of the 'encoder' part will not update when exploited in the downstream tasks. Is it?
2. In section 2.4, line 208-212, does it means that SAWR concatenated with original word representations, while Tree-RNN and Tree-Linear only use the hidden vectors?
3. In section 2.3, line 190, it can be seen that the paper purposed to concatenated outputs of 5 layers, which is 3 LSTM outputs and 2 MLP outputs. But in the open-source code, the word embedding layer outputs also be concatenated.
4. In the open-source code, there are 4 kinds of tasks in respective packages, but the 'sentence_classification' and 'sentence_matching' do not have the Tree-Linearization subpackage of baseline, while 'machine_translation' do not have Tree-Linearization subpackage.